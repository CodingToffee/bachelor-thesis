The implementation follows a staged approach to data generation and validation. Phase 1 employs offline synthetic datasets to design KPIs and to verify computations against ground truth. Phase 2 uses an online simulator via an OPC UA mock to evaluate streaming behavior, end-to-end latency, and storage characteristics. Phase 3 targets on-machine integration as planned future work. This approach decouples progress from hardware availability, enables controlled experiments with ground truth, and preserves reproducibility through fixed seeds and configurable scenarios.
For phase 1, a Python script was developed. At the inception of the script, a series of parameters were established. These parameters encompass the duration of the shift, after how many hours the break starts, the duration of the break, timeframe of the simulated data and the step width with which the data is generated. To mimic the actual production it was decided to use random values for setup times, idle times, sewing times, breakdown durations, amount of breakdowns. For these random values, upper and lower limits were set at the beginning. The so generated data has the following structure. Between each shift, there is a period, where the worker is setting up the machine and the workplace, this is entirely idle time. Then the pattern described in chapter 4.7 is repeated in random intervals. Between each step of the pattern there is an idle event, because usually when looking at the pattern on a millisecond timeframe there will always be some time between each action. Between the patterns there sometimes appear downtime events and eventually one lunch break. In the script for each day first the data is created and saved as a csv file. Then it is transformed into the influxDB line protocol and then written in batches to the database. 
For phase 2 there were two python scripts implemented. The first is almost identical to the script from phase 1. It generates almost the same data and writes it to a csv script. One difference is, that the data is filled up to simulate a continous stream of data similar to that of the sewing machine. The filling up is realized through utilization of the forward fill method. This method fills the gaps based on the last state and creates values according to the sampling rate. Every 24 hours a new csv file with data for the next 24 hours is created. To be sure that always enough data is there, the creation is triggered a few hours before the timestamp of the last entry of the data of the last 24 hours. The second script takes the csv file, starts an OPC UA server and streams the data with the set sampling rate. It is constructed in a way that it can be started at any point in time. This works because it first searches the closest entry to the current time. 
